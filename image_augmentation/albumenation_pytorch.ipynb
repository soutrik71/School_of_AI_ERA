{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of Albumentations library for data augmentation with pytorch torchvision library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb\n",
    "import os,random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms,datasets\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic torch visison transformer\n",
    "torchvision_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [00:01<00:00, 104509967.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train1 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=torchvision_transform\n",
    ")\n",
    "test1 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=torchvision_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you are using normal datasets---\n",
    "\n",
    "```python\n",
    "class TorchvisionDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        file_path = self.file_paths[idx]\n",
    "\n",
    "        # Read an image with PIL\n",
    "        image = Image.open(file_path)\n",
    "\n",
    "        # application of transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "torchvision_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "torchvision_dataset = TorchvisionDataset(\n",
    "    file_paths=[\"./images/image_1.jpg\", \"./images/image_2.jpg\", \"./images/image_3.jpg\"],\n",
    "    labels=[1, 2, 3],\n",
    "    transform=torchvision_transform,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of albumentations instead of torch vision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_transformation = alb.Compose(\n",
    "    [\n",
    "        alb.Resize(32, 32),\n",
    "        alb.HorizontalFlip(),\n",
    "        alb.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [00:01<00:00, 103895396.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train2 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=alb_transformation\n",
    ")\n",
    "test2 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=alb_transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "50000\n",
      "<class 'numpy.ndarray'>\n",
      "120.70756512369792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.1500758911213\n",
      "18540682003\n"
     ]
    }
   ],
   "source": [
    "print(train2.data.shape)\n",
    "print(len(train2.targets))\n",
    "print(type(train2.data))\n",
    "print(np.mean(train2.data))\n",
    "print(np.std(train2.data))\n",
    "print(np.sum(train2.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "50000\n",
      "<class 'numpy.ndarray'>\n",
      "120.70756512369792\n",
      "64.1500758911213\n",
      "18540682003\n"
     ]
    }
   ],
   "source": [
    "print(train1.data.shape)\n",
    "print(len(train1.targets))\n",
    "print(type(train1.data))\n",
    "print(np.mean(train1.data))\n",
    "print(np.std(train1.data))\n",
    "print(np.sum(train1.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hence it proved that we can use albumenation transforms inplace of torchvision transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class AlbumentationsPilDataset(Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        file_path = self.file_paths[idx]\n",
    "\n",
    "        image = Image.open(file_path)\n",
    "\n",
    "        if self.transform:\n",
    "            # Convert PIL image to numpy array\n",
    "            image_np = np.array(image)\n",
    "            # Apply transformations\n",
    "            augmented = self.transform(image=image_np)\n",
    "            # Convert numpy array to PIL Image\n",
    "            image = Image.fromarray(augmented['image'])\n",
    "        return image, label\n",
    "\n",
    "\n",
    "albumentations_pil_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.RandomCrop(224, 224),\n",
    "    A.HorizontalFlip(),\n",
    "])\n",
    "\n",
    "\n",
    "# Note that this dataset will output PIL images and not numpy arrays nor PyTorch tensors\n",
    "albumentations_pil_dataset = AlbumentationsPilDataset(\n",
    "    file_paths=['./images/image_1.jpg', './images/image_2.jpg', './images/image_3.jpg'],\n",
    "    labels=[1, 2, 3],\n",
    "    transform=albumentations_pil_transform,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Albumentations equivalents for torchvision transforms::\n",
    "\n",
    "torchvision transform\t||Albumentations transform||\tAlbumentations example\n",
    "Compose\t||Compose\t||A.Compose([A.Resize(256, 256), A.RandomCrop(224, 224)])\n",
    "CenterCrop\t||CenterCrop||\tA.CenterCrop(256, 256)\n",
    "ColorJitter\t||HueSaturationValue||\tA.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5)\n",
    "Pad\t||PadIfNeeded||\tA.PadIfNeeded(min_height=512, min_width=512)\n",
    "RandomAffine||\tAffine||\tA.Affine(scale=(0.9, 1.1), translate_percent=(0.0, 0.2), rotate=(-45, 45), shear=(-15, 15), mode=cv2.BORDER_REFLECT_101, p=0.5)\n",
    "RandomCrop||\tRandomCrop||\tA.RandomCrop(256, 256)\n",
    "RandomGrayscale||\tToGray||\tA.ToGray(p=0.5)\n",
    "RandomHorizontalFlip||\tHorizontalFlip||\tA.HorizontalFlip(p=0.5)\n",
    "RandomPerspective\t||Perspective\t||A.Perspective(scale=(0.2, 0.4), fit_output=True, p=0.5)\n",
    "RandomRotation||\tRotate||\tA.Rotate(limit=45, p=0.5)\n",
    "RandomVerticalFlip||\tVerticalFlip||\tA.VerticalFlip(p=0.5)\n",
    "Resize||\tResize||\tA.Resize(256, 256)\n",
    "GaussianBlur||\tGaussianBlur||\tA.GaussianBlur(blur_limit=(3, 7), p=0.5)\n",
    "RandomInvert\t||InvertImg||\tA.InvertImg(p=0.5)\n",
    "RandomPosterize||\tPosterize||\tA.Posterize(num_bits=4, p=0.5)\n",
    "RandomSolarize||\tSolarize||\tA.Solarize(threshold=127, p=0.5)\n",
    "RandomAdjustSharpness||\tSharpen\t||A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.5)\n",
    "RandomAutocontrast\t||RandomBrightnessContrast||\tA.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2, p=0.5)\n",
    "RandomEqualize||\tEqualize||\tA.Equalize(p=0.5)\n",
    "RandomErasing||\tCoarseDropout||\tA.CoarseDropout(min_height=8, max_height=32, min_width=8, max_width=32, p=0.5)\n",
    "Normalize||\tNormalize||\tA.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very Imp References:\n",
    "\n",
    "https://albumentations.ai/docs/examples/pytorch_classification/\n",
    "\n",
    "https://towardsdatascience.com/getting-started-with-albumentation-winning-deep-learning-image-augmentation-technique-in-pytorch-47aaba0ee3f8\n",
    "\n",
    "https://debuggercafe.com/image-augmentation-using-pytorch-and-albumentations/\n",
    "\n",
    "https://www.youtube.com/watch?v=rAdLwKJBvPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
