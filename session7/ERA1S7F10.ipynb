{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soutrik71/School_of_AI_ERA/blob/main/session7/ERA1S7F10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the best model with Image Augmentation we add LR scheduler"
      ],
      "metadata": {
        "id": "ewtmG-kXBt-l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtssFUKb-jqx"
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A84rlfDA23",
        "outputId": "d3ea9731-ec08-4dca-f14c-3dc0c87dcb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 454160963.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 15126834.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 216493704.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5772887.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OLDR79DrHG",
        "outputId": "7989e0a3-6e7a-4dc4-96cd-e2766e5c53bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQL3H6RJL3h"
      },
      "source": [
        "# The model\n",
        "Let's start with the model we first saw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FXQlB9kH1ov"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "dropout_value = 0.1\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 26\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 24\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 24\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 10\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 8\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 6\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 6\n",
        "\n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=6)\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            # nn.BatchNorm2d(10),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock8(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-vp8X9LCWo"
      },
      "source": [
        "# Model Params\n",
        "Can't emphasize on how important viewing Model Summary is.\n",
        "Unfortunately, there is no in-built model visualizer, so we have to take external help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skB97zIJQQe",
        "outputId": "f180fe76-0de8-4068-aba3-99000c4ee352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             144\n",
            "              ReLU-2           [-1, 16, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 16, 26, 26]              32\n",
            "           Dropout-4           [-1, 16, 26, 26]               0\n",
            "            Conv2d-5           [-1, 32, 24, 24]           4,608\n",
            "              ReLU-6           [-1, 32, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 32, 24, 24]              64\n",
            "           Dropout-8           [-1, 32, 24, 24]               0\n",
            "            Conv2d-9           [-1, 10, 24, 24]             320\n",
            "        MaxPool2d-10           [-1, 10, 12, 12]               0\n",
            "           Conv2d-11           [-1, 16, 10, 10]           1,440\n",
            "             ReLU-12           [-1, 16, 10, 10]               0\n",
            "      BatchNorm2d-13           [-1, 16, 10, 10]              32\n",
            "          Dropout-14           [-1, 16, 10, 10]               0\n",
            "           Conv2d-15             [-1, 16, 8, 8]           2,304\n",
            "             ReLU-16             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-17             [-1, 16, 8, 8]              32\n",
            "          Dropout-18             [-1, 16, 8, 8]               0\n",
            "           Conv2d-19             [-1, 16, 6, 6]           2,304\n",
            "             ReLU-20             [-1, 16, 6, 6]               0\n",
            "      BatchNorm2d-21             [-1, 16, 6, 6]              32\n",
            "          Dropout-22             [-1, 16, 6, 6]               0\n",
            "           Conv2d-23             [-1, 16, 6, 6]           2,304\n",
            "             ReLU-24             [-1, 16, 6, 6]               0\n",
            "      BatchNorm2d-25             [-1, 16, 6, 6]              32\n",
            "          Dropout-26             [-1, 16, 6, 6]               0\n",
            "        AvgPool2d-27             [-1, 16, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 13,808\n",
            "Trainable params: 13,808\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.06\n",
            "Params size (MB): 0.05\n",
            "Estimated Total Size (MB): 1.12\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs.\n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkF2nN_LYIb"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE5Le6FYHhc8",
        "outputId": "8fca9e0a-80bc-45a5-e9b8-888784d5cbff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=6, gamma=0.1) # addding of LR scheduler we can tryout other lRs as well as ReduceLRonPLateau\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13794854283332825 Batch_id=468 Accuracy=86.65: 100%|██████████| 469/469 [00:25<00:00, 18.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0607, Accuracy: 9824/10000 (98.24%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05200251564383507 Batch_id=468 Accuracy=97.69: 100%|██████████| 469/469 [00:23<00:00, 20.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0405, Accuracy: 9869/10000 (98.69%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04140068218111992 Batch_id=468 Accuracy=98.16: 100%|██████████| 469/469 [00:20<00:00, 22.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0336, Accuracy: 9903/10000 (99.03%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07808329910039902 Batch_id=468 Accuracy=98.38: 100%|██████████| 469/469 [00:21<00:00, 22.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0308, Accuracy: 9890/10000 (98.90%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0418878048658371 Batch_id=468 Accuracy=98.54: 100%|██████████| 469/469 [00:21<00:00, 21.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0332, Accuracy: 9882/10000 (98.82%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07754498720169067 Batch_id=468 Accuracy=98.64: 100%|██████████| 469/469 [00:22<00:00, 21.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0242, Accuracy: 9922/10000 (99.22%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.061365317553281784 Batch_id=468 Accuracy=98.88: 100%|██████████| 469/469 [00:23<00:00, 20.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0207, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.011696522124111652 Batch_id=468 Accuracy=98.99: 100%|██████████| 469/469 [00:22<00:00, 20.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0214, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.027557894587516785 Batch_id=468 Accuracy=98.96: 100%|██████████| 469/469 [00:22<00:00, 21.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.057884007692337036 Batch_id=468 Accuracy=99.00: 100%|██████████| 469/469 [00:21<00:00, 21.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04202042147517204 Batch_id=468 Accuracy=99.00: 100%|██████████| 469/469 [00:22<00:00, 21.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 9926/10000 (99.26%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.026343949139118195 Batch_id=468 Accuracy=98.99: 100%|██████████| 469/469 [00:21<00:00, 21.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.006624260917305946 Batch_id=468 Accuracy=99.05: 100%|██████████| 469/469 [00:22<00:00, 21.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0202, Accuracy: 9933/10000 (99.33%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.016489429399371147 Batch_id=468 Accuracy=99.01: 100%|██████████| 469/469 [00:21<00:00, 22.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 9931/10000 (99.31%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.013116572052240372 Batch_id=468 Accuracy=99.06: 100%|██████████| 469/469 [00:21<00:00, 21.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 9931/10000 (99.31%)\n",
            "\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04433055594563484 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:21<00:00, 21.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0194, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.023286275565624237 Batch_id=468 Accuracy=99.05: 100%|██████████| 469/469 [00:21<00:00, 21.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.01250071544200182 Batch_id=468 Accuracy=99.05: 100%|██████████| 469/469 [00:20<00:00, 22.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.038351912051439285 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:20<00:00, 23.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.00885955523699522 Batch_id=468 Accuracy=99.02: 100%|██████████| 469/469 [00:21<00:00, 22.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 9930/10000 (99.30%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drokW8wWODKq"
      },
      "source": [
        "# Let's Train and test our model\n",
        "\n",
        "This time let's add a scheduler for out LR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87RaqGSEOWDe",
        "outputId": "f4a5a577-d979-4207-bbaf-422a41be45e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "axs[0, 0].plot(train_losses)\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[1, 0].plot(train_acc[4000:])\n",
        "axs[1, 0].set_title(\"Training Accuracy\")\n",
        "axs[0, 1].plot(test_losses)\n",
        "axs[0, 1].set_title(\"Test Loss\")\n",
        "axs[1, 1].plot(test_acc)\n",
        "axs[1, 1].set_title(\"Test Accuracy\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-37ac7d30ffb5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             len(x.shape) < 1):\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target:\\\n",
        "Add LR Scheduler\n",
        "\n",
        "Results:\\\n",
        "Parameters: 13.8k\\\n",
        "Best Train Accuracy: 99.21\\\n",
        "Best Test Accuracy: 99.45 (9th Epoch), 99.48 (20th Epoch)\\\n",
        "\n",
        "Analysis:\\\n",
        "Finding a good LR schedule is hard. We have tried to make it effective by reducing LR by the 10th after the 6th epoch.\\\n",
        "It did help in getting to 99.4 or faster, but the final accuracy is not more than 99.5. Possibly a good scheduler can do wonders here!\\"
      ],
      "metadata": {
        "id": "wjO3RK9UEnvF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfIrSm2jCFR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}